\documentclass{article}
\usepackage{graphicx}
\usepackage{listings}
\lstset{basicstyle=\tt}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{url}
\usepackage{xspace}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{threeparttable}

\usepackage{pifont}
\newcommand{\tick}{\ding{52}}

\usepackage[hidelinks]{hyperref}
\usepackage{flushend}

\usepackage[margin=1in]{geometry}

\usepackage{tabu}

\let\mc\multicolumn

%\makeatletter
%\let\@copyrightspace\relax
%\makeatother

%%commands for centering lstlistings
\newcommand\lststart[1]{\begin{center}\begin{minipage}{#1}\lstset{basicstyle=\tt}}
\newcommand\lstend{\end{minipage}\end{center}}

\newcommand\kernel[1]{\phantomsection \addcontentsline{toc}{subsubsection}{{\tt #1}} \subsubsection*{{\tt #1}}}

\newcommand\polybenchversion{4.1\xspace}

\title{PolyBench \polybenchversion}
\author{
  Tomofumi Yuki\\
  Inria / LIP / ENS Lyon\\
  \texttt{tomofumi.yuki@inria.fr}
  \and
  Louis-No\"{e}l Pouchet\\
  Ohio State University\\
  \texttt{pouchet@cse.ohio-state.edu}
}


\begin{document}
\maketitle

\setcounter{tocdepth}{3}

\tableofcontents

\sloppy

\section{PolyBench \polybenchversion Kernels}
This document describes computations performed in PolyBench kernels from
mathematical stand point. The description is detached from implementation
decisions to provide a single and consistent point of reference for
implementations in various languages.


\subsection{Data Mining}
\kernel{covariance}

Computes the covariance, a measure from statistics that show how linearly related two variables are.

It takes the following as input,
\begin{itemize}
\item {\tt data}: $N\times M$ matrix that represents $N$ data points, each with $M$ attributes,
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt cov}: $M\times M$ matrix where the $i,j$-th element is the covariance between $i$ and $j$. The matrix is symmetric.
\end{itemize}

Covariance is defined to be the mean of the product of deviations for $i$ and $j$:
\[\displaystyle \mathtt{cov}(i,j) = \frac{\displaystyle \sum_{k=0}^{N-1}\left(\mathtt{data}(k,i) - \mathtt{mean}(i))(\mathtt{data}(k,j)-\mathtt{mean}(j)\right)}{N-1}\]
where
\[\displaystyle \mathtt{mean}(x) = \frac{\displaystyle \sum_{k=0}^{N-1}\mathtt{data}(k,x)}{N}\]
Note that the above computes \emph{sample covariance} where the denominator is $N-1$.  

\kernel{correlation}
Correlation computes the correlation coefficients (Pearson's), which is
normalized covariance.


It takes the following as input,
\begin{itemize}
\item {\tt data}: $N\times M$ matrix that represents $N$ data points, each with $M$ attributes,
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt corr}: $M\times M$ matrix where the $i,j$-th element is the correlation coefficient between $i$ and $j$. The matrix is symmetric.
\end{itemize}

Correlation is defined as the following,
\[\displaystyle \mathtt{corr}(i,j) = \frac{\mathtt{cov}(i,j)}{\mathtt{stddev}(i)\mathtt{stddev}(j)}\]
where
\[\displaystyle \mathtt{stddev}(x) = \sqrt{\frac{\displaystyle \sum_{k=0}^{N-1}\left(\mathtt{data}(k,x)-\mathtt{mean}(x)\right)^2}{N}}\]
{\tt cov} and {\tt mean} are defined in {\tt covariance}\footnote{However, the denominator when computing covariance is $N$ for correlation.}.


\subsection{BLAS Routines}

\kernel{gemm}
Generalized Matrix Multiply from BLAS~\cite{dongarra1990set}.

It takes the following as inputs,
\begin{itemize}
\item $\alpha, \beta$: scalars
\item {\tt A}: $P\times Q$ matrix
\item {\tt B}: $Q\times R$ matrix
\item {\tt C}: $P\times R$ matrix
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt Cout}: $P\times R$ array, where $\mathtt{Cout} = \alpha AB + \beta C$
\end{itemize}

Note that the output {\tt Cout} is to be stored in place of the input array {\tt C}. The BLAS parameters used are
 {\tt TRANSA = TRANSB = `N'}, meaning both {\tt A} and {\tt B} are not transposed.

\kernel{gemver}
Multiple matrix-vector multiplication from updated BLAS~\cite{blackford2002updated}.
However it is not part of the current BLAS distribution.

It takes the following as inputs,
\begin{itemize}
\item $\alpha, \beta$: scalars
\item {\tt A}: $N\times N$ matrix
\item {\tt u1}, {\tt u2}, {\tt v1}, {\tt v2}, {\tt y}, {\tt z}: vectors of length $N$
\end{itemize}
and gives the following as outputs:
\begin{itemize}
\item {\tt A'}: $N\times N$ matrix, where $A' = A + u1.v1 + u2.v2$
\item {\tt x}: vector of length $N$, where $x = \beta A'y + z$
\item {\tt w}: vector of length $N$, where $w = \alpha A'x$
\end{itemize}

\kernel{gesummv}
Summed matrix-vector multiplications from updated BLAS~\cite{blackford2002updated}.
However it is not part of the current BLAS distribution.

It takes the following as inputs,
\begin{itemize}
\item $\alpha, \beta$: scalars
\item {\tt A, B}: $N\times N$ matrix
\item {\tt x}: vector of length $N$
\end{itemize}
and gives the following as outputs:
\begin{itemize}
\item {\tt y}: vector of length $N$, where $y = \alpha Ax + \beta Bx$
\end{itemize}

\kernel{symm}
Symmetric matrix matrix multiplication from BLAS~\cite{dongarra1990set}.

It takes the following as inputs,
\begin{itemize}
\item $\alpha, \beta$: scalars
\item {\tt A}: $M\times M$ symmetric matrix
\item {\tt B,C}: $M\times N$ matrices
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt Cout}: $M\times N$ matrix, where $\mathtt{Cout} = \alpha AB + \beta C$
\end{itemize}

Note that the output {\tt Cout} is to be stored in place of the input array
{\tt C}.  The matrix $A$ is stored as a triangular matrix in BLAS. The
configuration used are {\tt SIDE = `L'} and {\tt UPLO = `L'}, meaning the
multiplication is from the left, and the symmetric matrix is stored as a lower
triangular matrix.

\kernel{syrk}
Symmetric rank k update from updated BLAS~\cite{blackford2002updated}.

It takes the following as inputs,
\begin{itemize}
\item $\alpha, \beta$: scalars
\item {\tt A}: $N\times M$ matrix
\item {\tt C}: $N\times N$ symmetric matrix
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt Cout}: $N\times N$ matrix, where $\mathtt{Cout} = \alpha AA^T + \beta C$
\end{itemize}

Note that the output {\tt Cout} is to be stored in place of the input array {\tt C}.
The matrix $C$ is stored as a triangular matrix in BLAS, and the result is also triangular.
The configuration used are {\tt TRANS = `N'} and {\tt UPLO = `L'} meaning the
{\tt A} matrix is not transposed, and {\tt C} matrix stores the symmetric
matrix as lower triangular matrix.


\kernel{syr2k}
Symmetric rank 2k update from updated BLAS~\cite{blackford2002updated}.

It takes the following as inputs,
\begin{itemize}
\item $\alpha, \beta$: scalars
\item {\tt A,B}: $N\times M$ matrices
\item {\tt C}: $N\times N$ symmetric matrix
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt Cout}: $N\times N$ matrix, where $\mathtt{Cout} = \alpha AB^T + \alpha BA^T + \beta C$
\end{itemize}

Note that the output {\tt Cout} is to be stored in place of the input array {\tt C}.
The matrix $C$ is stored as a triangular matrix in BLAS, and the result is also triangular.
The configuration used are {\tt TRANS = `N'} and {\tt UPLO = `L'} meaning the
{\tt A} matrix is not transposed, and {\tt C} matrix stores the symmetric
matrix as lower triangular matrix.

\kernel{trmm}
Triangular matrix multiplication.

It takes the following as inputs,
\begin{itemize}
\item {\tt A}: $N\times N$ lower triangular matrix
\item {\tt B}: $N\times N$ matrix
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt Bout}: $N\times N$ matrix, where $\mathtt{Bout} = AB$
\end{itemize}

Note that the output {\tt Bout} is to be stored in place of the input array
{\tt B}.  The configurations used are {\tt SIDE   = `L'}, {\tt UPLO   = `L'},
{\tt TRANSA = `T'}, and {\tt DIAG   = `U'}, meaning the multiplication is from
the left, the matrix is lower triangular, untransposed with unit diagonal.



\subsection{Linear Algebra Kernels}

\kernel{2mm}
Linear algebra kernel that consists of two matrix multiplications.

It takes the following as inputs,
\begin{itemize}
\item $\alpha, \beta$: scalars
\item {\tt A}: $P\times Q$ matrix
\item {\tt B}: $Q\times R$ matrix
\item {\tt C}: $R\times S$ matrix
\item {\tt D}: $P\times S$ matrix
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt E}: $P\times S$ matrix, where $E = \alpha ABC + \beta D$
\end{itemize}

\kernel{3mm}
Linear algebra kernel that consists of three matrix multiplications.

It takes the following as inputs,
\begin{itemize}
\item {\tt A}: $P\times Q$ matrix
\item {\tt B}: $Q\times R$ matrix
\item {\tt C}: $R\times S$ matrix
\item {\tt D}: $S\times T$ matrix
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt G}: $P\times T$ matrix, where $G = (A.B).(C.F)$
\end{itemize}

\kernel{atax}
Computes $A^T$ times $Ax$.

It takes the following as inputs,
\begin{itemize}
\item {\tt A}: $M\times N$ matrix
\item {\tt x}: vector of length $N$
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt y}: vector of length $N$, where $y = A^T(Ax)$
\end{itemize}


\kernel{bicg}
Kernel of BiCGSTAB (BiConjugate Gradient STABilized method).

It takes the following as inputs,
\begin{itemize}
\item {\tt A}: $N\times M$ matrix
\item {\tt p}: vector of length $M$
\item {\tt r}: vector of length $N$
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt q}: vector of length $N$, where $q = Ap$
\item {\tt s}: vector of length $M$, where $s = A^Tr$
\end{itemize}


\kernel{doitgen}
Kernel of Multiresolution ADaptive NumErical Scientific Simulation
(MADNESS)\footnote{\url{http://www.csm.ornl.gov/ccsg/html/projects/madness.html}}.
The kernel is taken from the modified version used by You et al.~\cite{you2007empirical}.

It takes the following as inputs,
\begin{itemize}
\item {\tt A}: $R\times Q\times S$ array
\item {\tt x}: $P\times S$ array
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt Aout}: $R\times Q\times P$ array
\end{itemize}

\[\mathtt{A}(r,q,p) = \displaystyle \sum_{s=0}^{S-1}\mathtt{A}(r,q,s)\mathtt{x}(p,s)\]

Note that the output {\tt Aout} is to be stored in place of the input array {\tt
A} in the original code. Although it is not mentioned anywhere, the computation does not make sense
if $P\neq S$. 

\kernel{mvt}
Matrix vector multiplication composed with another matrix vector multiplication but with transposed matrix.

It takes the following as inputs,
\begin{itemize}
\item {\tt A}: $N\times N$ matrix
\item {\tt y1, y2}: vectors of length $N$
\end{itemize}
and gives the following as outputs:
\begin{itemize}
\item {\tt x1}: vector of length $N$, where $x1 = x1 + Ay1$
\item {\tt x2}: vector of length $N$, where $x2 = x2 + A^Ty2$
\end{itemize}

\subsection{Linear Algebra Solvers}

\kernel{cholesky}
Cholesky decomposition, which decomposes a matrix to triangular matrices. Only
applicable when the input matrix is positive-definite.

It takes the following as input,
\begin{itemize}
\item {\tt A}: $N\times N$ positive-definite matrix
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt L}: $N\times N$ lower triangular matrix such that $A = LL^T$
\end{itemize}



The {\tt C} reference implementation uses Choleskyâ€“Banachiewicz algorithm. The
algorithm computes the following, where the computation starts from the
upper-left corner of $L$ and proceeds row by row.
\[\mathtt{L}(i,j) = \begin{cases}
i=j & : \displaystyle \sqrt{\mathtt{A}(i,i)-\sum_{k=0}^{i-1}\mathtt{L}(i,k)^2} \\
i>j & : \displaystyle \frac{\displaystyle \mathtt{A}(i,j) - \sum_{k=0}^{j-1} \mathtt{L}(i,k)\mathtt{L}(j,k)}{\mathtt{L}(j,j)}
\end{cases}\]


\kernel{durbin}
Durbing is an algorithm for solving Yule-Walker equations, which is a special case of Toelitz systems.

It takes the following as input,
\begin{itemize}
\item {\tt r}: vector of length $N$.
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt y}: vector of length $N$
\end{itemize}
such that $Ty = -r$ where $T$ is a symmetric, unit-diagonal, Toeplitz matrix
defined by the vector $[1, r_0, ..., r_{N-1}]$.

The C reference implementation is a direct implementation of the algorithm
described in a book by Golub and Van Loan~\cite{golub2012matrix}. The book
mentions that a vector can be removed to use less space, but the implementation
retains this vector.

\kernel{gramschmidt}
QR Decomposition with Modified Gram Schmidt.

It takes the following as input,
\begin{itemize}
\item {\tt A}: $M\times N$ rank $N$ matrix ($M > N$).
\end{itemize}
and gives the following as outputs:
\begin{itemize}
\item {\tt Q}: $M\times N$ orthogonal matrix
\item {\tt R}: $N\times N$ upper triangular matrix
\end{itemize}
such that $A = QR$.

The algorithm is described in a techreport by Walter Gander:
\url{http://www.inf.ethz.ch/personal/gander/}.

\kernel{lu}
LU decomposition without pivoting.

It takes the following as input,
\begin{itemize}
\item {\tt A}: $N\times N$ matrix
\end{itemize}
and gives the following as outputs:
\begin{itemize}
\item {\tt L}: $N\times N$ lower triangular matrix
\item {\tt U}: $N\times N$ upper triangular matrix
\end{itemize}
such that $A = LU$.

{\tt L} and {\tt U} are computed as follows:

\[\mathtt{U}(i,j) = \mathtt{A}(i,j) - \displaystyle \sum_{k=0}^{i-1} \mathtt{L}(i,k)\mathtt{U}(k,j)\]
\[\mathtt{L}(i,j) = \displaystyle \frac{\mathtt{A}(i,j) - \displaystyle \sum_{k=0}^{j-1} \mathtt{L}(i,k)\mathtt{U}(k,j)}{\mathtt{U}(j,j)}\]


\kernel{ludcmp}
This kernel solves a system of linear equations using LU decomposition followed
by forward and backward substitutions.

It takes the following as inputs,
\begin{itemize}
\item {\tt A}: $N\times N$ matrix
\item {\tt b}: vector of length $N$
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt x}: vector of length $N$, where $Ax = b$
\end{itemize}

The matrix $A$ is first decomposed into $L$ and $U$ using the same algorithm as in {\tt lu}.
Then the two triangular systems are solved to find $x$ as follows:
\[ Ax = b \Rightarrow LUx = b \Rightarrow
\begin{cases}
Ly = b\\
Ux = y
\end{cases}\]

The forward and backward substitutions are as follows:
\[\mathtt{y}(i) = \displaystyle \frac{\mathtt{b}(i) - \displaystyle \sum_{j=0}^{i-1} \mathtt{L}(i,j)\mathtt{y}(j) }{\mathtt{L}(i,i)}\]
\[\mathtt{x}(i) = \displaystyle \frac{\mathtt{y}(i) - \displaystyle \sum_{j=0}^{i-1} \mathtt{U}(i,j)\mathtt{x}(j) }{\mathtt{U}(i,i)}\]


\kernel{trisolv}
Triangular matrix solver using forward substitution.

It takes the following as inputs,
\begin{itemize}
\item {\tt L}: $N\times N$ lower triangular matrix
\item {\tt b}: vector of length $N$
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt x}: vector of length $N$, where $Lx=b$
\end{itemize}

The forward substitution is as follows:
\[\mathtt{x}(i) = \displaystyle \frac{\mathtt{b}(i) - \displaystyle \sum_{j=0}^{i-1} \mathtt{L}(i,j)\mathtt{x}(j) }{\mathtt{L}(i,i)}\]


\subsection{Medley}

\kernel{deriche}
{\tt deriche} implements the Deriche recursive filter, which is a generic filter that can be used for both
edge detection and smoothing~\cite{deriche1987using, deriche1990fast}. The 2D version take advantage of
the separability and is implemented as horizontal passes followed by vertical passes over an image.

It takes the following as input,
\begin{itemize}
\item {\tt x}: $W\times H$ image.
\item $\alpha$: A parameter of the filter that is related to the size of the convolution.
\item $a_1, \dots, a_8, b_1, b_2, c_1, c_2$: Coefficients of the filter that determine the behavior of the filter.
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt y}: The processed image. How the image is processed depends on the coefficients used.
\end{itemize}

The generic 2D implementation reproduced from the original article~\cite{deriche1990fast} is as follows:
\paragraph{Horizontal Pass}
\[y_1(i,j) = a_1 \mathtt{x}(i,j) + a_2 \mathtt{x}(i,j-1) + b_1 y_1(i,j-1) + b_2 y_1(i,j-2)\]
\[y_2(i,j) = a_3 \mathtt{x}(i,j+1) + a_4 \mathtt{x}(i,j+2) + b_1 y_2(i,j+1) + b_2 y_2(i,j+2)\]
\[r(i,j) = c_1\left(y_1(i,j) + y_2(i,j)\right)\]

\paragraph{Vertical Pass}
\[z_1(i,j) = a_5 r(i,j) + a_6 r(i-1,j) + b_1 y_1(i-1,j) + b_2 y_1(i-2,j)\]
\[z_2(i,j) = a_7 r(i+1,j) + a_8 r(i+2,j) + b_1 y_1(i+1,j) + b_2 y_1(i+2,j)\]
\[\mathtt{y}(i,j) = c_2\left(z_1(i,j) + z_2(i,j)\right)\]

The C implementation in PolyBench use the following parameters for the filter:
\begin{itemize}
\item $\alpha = 0.25$
\item $a_1 = a_5 = k$
\item $a_2 = a_6 = ke^{-\alpha}(\alpha -1)$
\item $a_3 = a_7 = ke^{-\alpha}(\alpha +1)$
\item $a_4 = a_8 = -ke^{-2\alpha}$
\item $b_1 = 2e^{-\alpha}$
\item $b_2 = -e^{-2\alpha}$
\item $c_1 = c_2 = 1$
\end{itemize}
where $k$ is computed as 
\[k = \frac{(1 - e^{-\alpha})^2}{1+2\alpha e^{-\alpha} - e^{-2\alpha}}\]
which implements the smoothing filter.

\kernel{floyd-warshall}
Floyd-Warshall computes the shortest paths between each pair of nodes in a
graph.  The following only computes the shortest path lengths, which is a typical
use of this method.

It takes the following as input,
\begin{itemize}
\item {\tt w}: $N\times N$ matrix, where the $i,j$th entry represents the cost of taking an edge from $i$ to $j$.
Set to infinity if there is no edge connecting $i$ to $j$.
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt paths}: $N\times N$ matrix, where the $i,j$th entry represents the shortest path length from $i$ to $j$.
\end{itemize}

The shortest path lengths are computed recursively as follows:
\[\mathtt{p}(k,i,j) = \begin{cases}
 k=-1 & : \mathtt{w}(i,j) \\
 0\le k < N & : min(\mathtt{p}(k-1,i,j), \mathtt{p}(k-1,i,k) + \mathtt{p}(k-1,k,j)
\end{cases}\]
where the final output $\mathtt{paths}(i,j) = \mathtt{p}(N-1,i,j)$.

\kernel{nussinov}
Nussinov is an algorithm for predicting RNA folding, and is an instance of dynamic programming.

It takes the following as input,
\begin{itemize}
\item {\tt seq}: RNA sequence of length $N$. The valid entries are one of `A' `G' `C' `T'. (or `U' in place of `T').
\end{itemize}
and gives the following as output:
\begin{itemize}
\item {\tt table}: $N\times N$ triangular matrix, which is the dynamic programming table.
\end{itemize}

The table is filled using the following formula:
\[\mathtt{table}(i,j) = \mathit{max} \left(
\begin{array} {c}
\mathtt{table}(i+1,j)\\
\mathtt{table}(i,j-1)\\
\mathtt{table}(i+1,j-1) + w(i,j)\\
\mathit{max}_{i<k<j}\left( \mathtt{table}(i,k) + \mathtt{table}(k+1,j) \right)\\
\end{array}\right)
\]
where $w$ is the scoring function that evaluate the pair of sequences
$\mathtt{seq}[i]$ and $\mathtt{seq}[j]$.  For Nussinov algorithm, the scoring
function returns 1 if the sequences are complementary (either `A' with `T' or
`G' with `C'), and 0 otherwise.


\subsection{Stencils}
Stencil computations iteratively update a grid of data using the same pattern
of computation.  For stencil computations, we denote the time step of the
iterative process as super scripts to simplify the description.

In the reference implementations, boundary conditions are simplified by not
updating them. The updates are only performed when all of its operands are
available (i.e., no out-of-bounds accesses).

\kernel{adi}
Alternating Direction Implicit method for 2D heat diffusion.
The main strength of ADI over other methods is that it decomposes a 2D problem
to two 1D problems, allowing direct solutions to the sub-problems with lower cost.
For 2D heat equations, each sub-problem becomes a tridiagonal system of equations.

From a ``stencil'' point of view, this can be seen as taking the heat equation of the form:
 \[\mathtt{u}^{t+1} = f\left(\begin{array}{ccc}
   & \mathtt{u}^t_{i,j+1}, & \\
   \mathtt{u}^t_{i+1,j}, & \mathtt{u}^t_{i,j}, & \mathtt{u}^t_{i-1,j}\\
   & \mathtt{u}^t_{i,j-1} &
 \end{array}
 \right)\]
and splitting it two two steps:
 \[\mathtt{u}^{t+\frac{1}{2}} = g\left( \mathtt{u}^t_{i+1,j}, \mathtt{u}^t_{i,j}, \mathtt{u}^t_{i-1,j} \right)\]
 \[\mathtt{u}^{t+1} = h\left( \mathtt{u}^{t+\frac{1}{2}}_{i,j+1}, \mathtt{u}^{t+\frac{1}{2}}_{i,j}, \mathtt{u}^{t+\frac{1}{2}}_{i,j-1} \right)\]


The C reference implementation is based on a Fortran code in a paper by Li and
Kedem~\cite{lee2002automatic}.  In this implementation, the coefficients are
also computed since the parameters configure the resolution and not the size of
the space/time domain.


\kernel{fdtd-2d}
Simplified Finite-Difference Time-Domain method for 2D data, which models
electric and magnetic fields based on Maxwell's equations. In particular,
the polarization used here is $TE^z$; Transverse Electric in $z$ direction.
It is a stencil involving three variables, {\tt Ex}, {\tt Ey}, and {\tt Hz}.
{\tt Ex} and {\tt Ey} are electric fields varying in $x$ and $y$ axes, where
{\tt Hz} is the magnetic field along $z$ axis. Fields along other axes are
either zero or static, and are not modeled.

\begin{align*}
\mathtt{Hz}^{t}_{(i,j)} & = C_{hzh} \mathtt{Hz}^{t-1}_{(i,j)} + C_{hze} (\mathtt{Ex}^{t-1}_{(i,j+1)} - \mathtt{Ex}^{t-1}_{(i,j)} - \mathtt{Ey}^{t-1}_{(i+1,j)} - \mathtt{Ey}^{t-1}_{(i,j)})\\
\mathtt{Ex}^{t}_{(i,j)} & = C_{exe} \mathtt{Ex}^{t-1}_{(i,j)} + C_{exh} (\mathtt{Hz}^{t}_{(i,j)} - \mathtt{Hz}^{t}_{(i,j-1)})\\
\mathtt{Ey}^{t}_{(i,j)} & = C_{eye} \mathtt{Ey}^{t-1}_{(i,j)} + C_{eyh} (\mathtt{Hz}^{t}_{(i,j)} - \mathtt{Hz}^{t}_{(i-1,j)})
\end{align*}

Variables $C_{xxx}$ are coefficients that may be different depending on the
location within the descretized space.  In PolyBench, it is simplified as
scalar coefficients.

\kernel{heat-3d}
Heat Equation over 3D space. The main update is as follows:

\[\mathtt{data}^{t}_{(i,j,k)} = \mathtt{data}^{t-1}_{(i,j,k)} +
\left(\begin{array}{l}
0.125 \left(\mathtt{data}^{t-1}_{(i+1,j,k)} -2 \mathtt{data}^{t-1}_{(i,j,k)} + \mathtt{data}^{t-1}_{(i-1,j,k)}\right) + \\
0.125 \left(\mathtt{data}^{t-1}_{(i,j+1,k)} -2 \mathtt{data}^{t-1}_{(i,j,k)} + \mathtt{data}^{t-1}_{(i,j-1,k)}\right) + \\
0.125 \left(\mathtt{data}^{t-1}_{(i,j,k+1)} -2 \mathtt{data}^{t-1}_{(i,j,k)} + \mathtt{data}^{t-1}_{(i,j,k-1)}\right) 
\end{array}\right)
\]

The C reference implementation is originally from Pochoir distribution.


\kernel{jacobi-1d}
Jacobi style stencil computation over 1D data with 3-point stencil pattern. It
originally comes from the Jacobi method for solving system of equations.
However, Jacobi-style stencils may refer to computations with some stencil
pattern that use values computed in the previous time step, which is a
characteristic of Jacobi method. The computation in PolyBench is simplified as
simply taking the average of three points.


\[\mathtt{data}^{t}_{(i)} = 0.33333 * \left(\mathtt{data}^{t-1}_{(i)} + \mathtt{data}^{t-1}_{(i-1)} + \mathtt{data}^{t-1}_{(i+1)}\right)\]



\kernel{jacobi-2d}
Jacobi-style stencil computation over 2D data with 5-point stencil pattern.
The computation is simplified as simply taking the average of five points.

\[\mathtt{data}^{t}_{(i,j)} = 0.2 * \left(\mathtt{data}^{t-1}_{(i,j)} + \mathtt{data}^{t-1}_{(i-1,j)} + 
  \mathtt{data}^{t-1}_{(i+1,j)} + \mathtt{data}^{t-1}_{(i,j-1)} + \mathtt{data}^{t-1}_{(i,j+1)}\right)\]

\kernel{seidel-2d}
Gauss-Seidel style stencil computation over 2D data with 9-point stencil
pattern.  Similar to Jacobi style stencils, Gauss-Seidel style stencil comes
from Gauss-Seidel method for solving systems of linear equations. The main
difference is that values computed at the same time step is used, which changes
its convergence and other properties.  The computation in PolyBench is
simplified as simply taking the average of nine points.

\[\mathtt{data}^{t}_{(i,j)} = \left(
\begin{array}{c}
   \mathtt{data}^{t-1}_{(i-1,j-1)} + \mathtt{data}^{t-1}_{(i-1,j)} +  \mathtt{data}^{t-1}_{(i-1,j+1)} + \\
   \mathtt{data}^{t-1}_{(i,j-1)} + \mathtt{data}^{t-1}_{(i,j)} +  \mathtt{data}^{t-1}_{(i-1,j+1)} + \\
    \mathtt{data}^{t-1}_{(i+1,j-1)} + \mathtt{data}^{t-1}_{(i+1,j)} + \mathtt{data}^{t-1}_{(i+1,j+1)}
\end{array}\right)/9\]

\section{C Implementation}
The C implementation follows the algorithms described in this document. There are many different ways
to implement each of them, and some guidelines used when selecting the implementation details are given below.


\begin{itemize}
\item The general principle is to make the kernels as simple as possible for
research compilers to handle.  In 3.3, one exception is made and reverse loops
are now included in some of the kernels. The main motivation is that reverting
these loops makes the kernels much more unnatural and complicated to
understand. The dataflow analysis implemented in Integer Set Library allow
reverse loops to be handled very easily. 
\item For BLAS kernels, it is kept as close to the original Fortran implementation as possible. 
However, they are not identical in most cases due to the difference in array layout. Some BLAS
kernels use scalars preventing loops to be parallelized unless the scalars are privatized.
\item The memory allocations used in the implementation is intended to be
\emph{a} natural allocation for each kernel. As a rule of thumb, the
arrays are made as compact as possible, while keeping them rectangular.
Most dense linear algebra kernels performs in-place computation when applicable.
\end{itemize}

\subsection*{Known Issues}
\begin{itemize}
\item {\tt correlation} output is all 1 due to the property of correlation
combined with the input generation. (Values become very regular since they are
computed as a function of loop indices.)
\item {\tt gramschmidt} large part of the output matrix $R$ become 0 (even
considering it is triangular). The output can contain NaN, which also has to do
with how the inputs are generated.
\end{itemize}

\subsection*{Pre-defined Problem Sizes}
PolyBench/C \polybenchversion comes with 5 pre-defined problem sizes. The problem sizes are
primarily derived from the memory requirements such that different levels of
the memory hierarchy are exercised.

\begin{itemize}
\item {\tt MINI}: Less than 16KB of memory. The problem may fit within the L1 (last level) cache.
\item {\tt SMALL}: Around 128KB of memory. The problem should not fit within the L1 cache, but may fit L2.
\item {\tt MEDIUM}: Around 1MB of memory. The problem should not fit within the L2 cache, but may fit L3.
\item {\tt LARGE}: Around 25MB of memory. The problem should not fit within the L3 cache.
\item {\tt EXTRALARGE}: Around 120MB of memory. 
\end{itemize}

Additional remarks:
\begin{itemize}
\item The default problem size is {\tt LARGE}.
\item It is  recommended to revise your problem sizes when the data types are changed.
\item Different parameters are given different values by default.
\item Sizes for {\tt deriche} are taken from commonly used image resolutions.
\item Two kernels ({\tt durbin} and {\tt jacobi-1d}) have $O(n)$ memory requirement, and thus the above rules do not apply.
The problem sizes for these kernels match those of other solvers (e.g., {\tt cholesky}).
\end{itemize}

\subsection*{Summary of Characteristics}
Table~\ref{tab:summary} summarizes the number of operations and memory usage of
each kernel. Although PolyBench discourages evaluations using subsets of its
kernels, some kernels may not be relevant in some contexts. One possible use of
these characteristics are in helping the explanation of the selection (of
subsets), in combination with the categorization.

\begin{table}
\centering
\caption{\label{tab:summary}
Table of kernel characteristics. The problem size parameters are assumed to
take the same value $n$. Calls to math functions (e.g., {\tt sqrt}) are also
consider to be one operation. For kernels that use integers as the default
datatype ({\tt nussinov}, {\tt floyd-warshall}), the number of operations only
count those related to the main computation, and do not include those for iterating 
over loops.
}
\tabulinesep=2pt
\begin{tabu}{|c|r|r|r|r|}
\hline
kernel & \mc1{c|}{Operations} & \mc1{c|}{Memory} & \mc1{c|}{O(Ops)} & \mc1{c|}{O(Mem)}\\
\hline
{\tt correlation} & $n^3+8n^2+3n$ & $2n^2+2n$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt covariance} & $n^3+4n^2+2n$ & $2n^2+n$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt 2mm} & $5n^3+n^2$ & $4n^2$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt 3mm} & $6n^3$ & $4n^2$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt atax} & $4n^2$ & $n^2+3n$ & $O(n^2)$ & $O(n^2)$\\
\hline
{\tt bicg} & $4n^2$ & $n^2+4n$ & $O(n^2)$ & $O(n^2)$\\
\hline
{\tt doitgen} & $2n^4$ & $n^3+n^2+n$ & $O(n^4)$ & $O(n^3)$\\
\hline
{\tt mvt} & $4n^2$ & $n^2+4n$ & $O(n^2)$ & $O(n^2)$\\
\hline
{\tt gemm} & $3n^3+n^2$ & $3n^2$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt gemver} & $10n^2+n$ & $n^2+8n$ & $O(n^2)$ & $O(n^2)$\\
\hline
{\tt gesummv} & $4n^2+3n$ & $2n^2+3n$ & $O(n^2)$ & $O(n^2)$\\
\hline
{\tt symm} & $\frac{5}{2}n^3+\frac{5}{2}n^2$ & $3n^2$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt syr2k} & $6n^3+n^2$ & $3n^2$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt syrk} & $\frac{3}{2}n^3+2n^2+\frac{1}{2}n$ & $3n^2$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt trmm} & $n^3$ & $3n^2$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt cholesky} & $\frac{1}{3}n^3+\frac{1}{2}n^2+\frac{1}{6}n$ & $n^2$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt durbin} & $2n^2+3n-5$ & $2n$ & $O(n^2)$ & $O(n)$\\
\hline
{\tt gramschmidt} & $2n^3+n^2+n$ & $3n^2$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt lu} & $\frac{4}{3}n^3 - \frac{3}{2}n^2 +\frac{1}{6}n$ & $n^2$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt ludcmp} & $\frac{4}{3}n^3+\frac{1}{2}n^2 -\frac{5}{6}n$ & $n^2+3n$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt trisolv} & $n^2$ & $n^2+2n$ & $O(n^2)$ & $O(n^2)$\\
\hline
{\tt deriche} & $32n^2$ & $4n^2$ & $O(n^2)$ & $O(n^2)$\\
\hline
{\tt floyd-warshall} & $2n^3$ & $n^2$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt nussinov} & $\frac{1}{6}n^3 - \frac{1}{2}n^2 + \frac{1}{3}n$ & $n^2+n$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt adi} & $30n^3$ & $4n^2$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt fdtd-2d} & $11n^3$ & $3n^2+n$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt heat-3d} & $30n^4$ & $2n^3$ & $O(n^4)$ & $O(n^3)$\\
\hline
{\tt jacobi-1d} & $6n^2$ & $2n$ & $O(n^2)$ & $O(n)$\\
\hline
{\tt jacobi-2d} & $10n^3$ & $2n^2$ & $O(n^3)$ & $O(n^2)$\\
\hline
{\tt seidel-2d} & $9n^3$ & $n^2$ & $O(n^3)$ & $O(n^2)$\\
\hline
\end{tabu}
\end{table}

\bibliographystyle{abbrv}
\bibliography{polybench}



\end{document}
